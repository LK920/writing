---
aliases:
  - pub/sub은 비효율적이다?
tags:
---
---
# Redis 클러스터에서 PUBLISH 한 번이 네트워크를 20배 낭비하는 이유

## 들어가며

최근 "개발자를 위한 레디스" 책의 메시지 브로커 챕터를 읽던 중, 이런 문구를 발견했습니다:

> "Redis 클러스터 환경에서 Pub/Sub은 비효율적이다"

처음에는 "뭐가 문제지?"라는 생각이 들었습니다. Redis 클러스터는 데이터를 분산 저장해서 성능을 높이는 건데, Pub/Sub도 당연히 잘 동작할 거라고 생각했거든요. 하지만 자료를 찾아보니 생각보다 훨씬 심각한 구조적 문제가 있었습니다.

**핵심은 이겁니다:** Redis 클러스터에서 `PUBLISH` 명령을 한 번 실행하면, 그 메시지가 클러스터의 **모든 노드**로 브로드캐스트됩니다. 10개 노드 클러스터라면? 메시지 1개가 10개로 복제되어 전송됩니다. 20개 노드라면 20배의 네트워크 트래픽이 발생하죠.

이 글은 제가 이해한 내용을 정리하고 기억하기 위해 작성합니다. Redis 클러스터를 도입하려는 분들, 특히 Pub/Sub을 활용한 실시간 기능을 계획 중이라면 꼭 알아야 할 내용입니다.

## 목차

1. [Redis Pub/Sub의 기본 동작](#1-redis-pubsub의-기본-동작)
2. [Redis 클러스터에서 Pub/Sub의 문제점](#2-redis-클러스터에서-pubsub의-문제점)
3. [일반 데이터는 어떻게 저장될까](#3-일반-데이터는-어떻게-저장될까)
4. [핵심 차이점 정리](#4-핵심-차이점-정리)
5. [분산락에서 Pub/Sub 사용하기](#5-분산락에서-pubsub-사용하기)
6. [그렇다면 해결 방법은](#6-그렇다면-해결-방법은)

## 1. Redis Pub/Sub의 기본 동작

### 1.1 Pub/Sub은 "통로"일 뿐

Redis Pub/Sub의 가장 중요한 특징은 **메시지를 저장하지 않는다**는 점입니다.

```
[발행자] → PUBLISH channel "message" → [Redis] → [구독자]
                                          ↓
                                    메시지는 저장되지 않고 즉시 전달
```

**특징:**

- 메시지가 Redis를 "통과"만 함
- 저장 ✗ / 대기 ✗ / 재전송 ✗
- 현재 구독 중인 클라이언트에게만 전달

**구독자가 없으면?**

```bash
# 구독자 없는 상태에서 발행
PUBLISH notifications "중요한 메시지"
# → 메시지는 그냥 사라짐 (손실)
```

**구독자가 오프라인이면?**

```bash
# 시간 T1: 앱 A가 구독 중
SUBSCRIBE channel

# 시간 T2: 앱 A 연결 끊김

# 시간 T3: 메시지 발행
PUBLISH channel "message"
# → 앱 A는 받지 못함 (복구 불가)
```

### 1.2 라디오 vs 팟캐스트 비유

**Pub/Sub = 라디오 방송**

- 지금 듣고 있으면: 들림 ✓
- 듣고 있지 않으면: 못 들음 ✗
- 다시 듣기: 불가능 ✗

**Streams/Kafka = 팟캐스트**

- 나중에 들어도: 들을 수 있음 ✓
- 재생 위치 저장: 가능 ✓
- 다시 듣기: 가능 ✓

## 2. Redis 클러스터에서 Pub/Sub의 문제점

### 2.1 전역 브로드캐스트

Redis 클러스터에서 Pub/Sub 메시지를 발행하면, **클러스터의 모든 노드로 메시지가 복제 전송**됩니다.

```
[애플리케이션]
    ↓ PUBLISH myChannel "hello" (1번만 전송)
[Redis Node 1] ← 메시지 수신
    ↓ ↓ ↓ ↓ ↓ (클러스터 내부 브로드캐스트)
[Node 2] [Node 3] [Node 4] [Node 5] [Node 6]
```

**왜 이렇게 동작할까?**

어떤 노드에 구독자가 연결되어 있을지 미리 알 수 없기 때문에, "모든 노드에 보내고 각 노드가 자신의 구독자에게 전달"하는 방식을 사용합니다.

### 2.2 구체적인 리소스 낭비

**시나리오: 6노드 클러스터, Node 1에만 구독자가 있는 경우**

```
PUBLISH myChannel "message"
↓
Node 1: 구독자 있음 → 메시지 전달 ✓
Node 2: 구독자 없음 → 메시지 받고 버림 ✗
Node 3: 구독자 없음 → 메시지 받고 버림 ✗
Node 4: 구독자 없음 → 메시지 받고 버림 ✗
Node 5: 구독자 없음 → 메시지 받고 버림 ✗
Node 6: 구독자 없음 → 메시지 받고 버림 ✗
```

**네트워크 부하:**

- 1개 메시지 → 6개 노드로 복제 전송
- 실제 필요: 1개 노드
- 낭비된 네트워크: 5배

**CPU 및 메모리:**

- 모든 노드가 메시지 파싱
- 구독자 리스트 조회
- 불필요한 처리 오버헤드

### 2.3 대규모 환경에서의 영향

```
환경:
- 클러스터 노드: 20개
- 초당 메시지: 10,000개
- 평균 메시지 크기: 1KB

실제 필요한 트래픽: 10,000 msg/sec × 1KB = 10 MB/sec
실제 발생 트래픽: 10,000 × 20 = 200,000 msg/sec × 1KB = 200 MB/sec

→ 20배의 네트워크 오버헤드 발생
```

### 2.4 물리적 배치에 따른 비용

Redis 클러스터의 각 노드는 보통 별도의 물리/가상 서버에서 실행됩니다:

```
[같은 데이터센터]
서버실 내부 네트워크 (1Gbps ~ 10Gbps)
- 레이턴시: 낮음
- 하지만 대역폭 공유로 인한 영향

[다른 AZ(Availability Zone)]
AZ-A: Node 1, 2
AZ-B: Node 3, 4  
AZ-C: Node 5, 6
→ AZ 간 네트워크 비용 발생 (AWS 등에서 과금)
→ 레이턴시 증가

[멀티 리전]
서울: Node 1, 2, 3
도쿄: Node 4, 5, 6
→ 국가 간 네트워크 전송 (매우 비쌈)
→ 큰 레이턴시
```

## 3. 일반 데이터는 어떻게 저장될까

Pub/Sub과 달리, 일반 데이터(SET, GET 등)는 **해시 슬롯에 따라 특정 노드에만 저장**됩니다.

### 3.1 해시 슬롯 기반 샤딩

```
Redis 클러스터: 16,384개의 해시 슬롯 (0-16383)

[클러스터 구성]
Master 1: 슬롯 0-5460
Master 2: 슬롯 5461-10922
Master 3: 슬롯 10923-16383
```

**데이터 저장 과정:**

```bash
SET lock:seat:A1 "value"

1. "lock:seat:A1" → CRC16 해시 계산 → 슬롯 12345
2. 슬롯 12345는 Master 3 담당

[저장되는 곳]
Master 3: ✓ (원본)
Replica 3: ✓ (복제본)

[저장 안 되는 곳]
Master 1, 2: ✗
Replica 1, 2: ✗
```

### 3.2 리다이렉션 메커니즘

**"잘못된 노드에 요청하면 어떻게 되나요?"**

Redis 클러스터는 자동 리다이렉션으로 이 문제를 해결합니다:

```
1. 클라이언트 → Master 1: GET lock:seat:A1

2. Master 1의 응답:
   -MOVED 12345 192.168.1.3:6379
   ("슬롯 12345는 Master 3에 있어요")

3. 클라이언트 → Master 3: GET lock:seat:A1 (재요청)
   → OK "value"
```

### 3.3 스마트 클라이언트 최적화

실무에서 사용하는 클라이언트(Lettuce, Jedis)는 **슬롯 맵을 캐싱**하여 처음부터 올바른 노드로 요청합니다:

```java
// Spring Boot + Lettuce
RedisClusterClient client = RedisClusterClient.create("redis://localhost:7001");
RedisAdvancedClusterCommands<String, String> commands = client.connect().sync();

// 클라이언트 내부 동작:
// 1. CLUSTER SLOTS 명령으로 슬롯 맵 조회
// 2. "lock:seat:A1" 해시 계산 → 슬롯 12345
// 3. 슬롯 맵에서 Master 3 확인
// 4. 처음부터 Master 3로 요청

commands.get("lock:seat:A1");  // 리다이렉션 없이 바로 성공
```

**슬롯 맵 캐싱:**

```
[클라이언트 메모리]
슬롯 맵:
0-5460      → Master 1 (192.168.1.1:6379)
5461-10922  → Master 2 (192.168.1.2:6379)
10923-16383 → Master 3 (192.168.1.3:6379)

매 요청마다:
hash(key) → 슬롯 번호 → 맵 조회 → 해당 마스터로 바로 요청
```

## 4. 핵심 차이점 정리

|구분|일반 데이터 (SET, GET)|Pub/Sub (PUBLISH)|
|---|---|---|
|**저장 위치**|해시 슬롯에 따라 특정 노드 1개|저장하지 않음|
|**노드 간 전파**|전파 안 됨 (복제본에만)|모든 노드로 브로드캐스트|
|**네트워크 효율**|높음 ✓|낮음 ✗|
|**샤딩**|지원 ✓|무시됨 ✗|

**전파 방식 비교:**

```
일반 데이터:
SET key value → 특정 노드 1개 (+ 복제본) → 샤딩 효율적 ✓

Pub/Sub:
PUBLISH channel msg → 모든 노드 브로드캐스트 → 비효율적 ✗
```

## 5. 분산락에서 Pub/Sub 사용하기

분산락 구현(예: Redisson)에서는 Pub/Sub을 보조적으로 사용합니다.

### 5.1 일반적인 분산락 (Spin Wait)

```java
// 비효율적: 계속 반복 시도 (CPU 낭비)
while (true) {
    if (tryLock("order:123")) {
        break;  // 락 획득 성공
    }
    Thread.sleep(100);  // 폴링
}
```

### 5.2 Pub/Sub을 활용한 효율적 대기

```java
// 1. 락 획득 시도
if (!tryLock("order:123")) {
    
    // 2. 실패 시 락 해제 알림 구독
    subscribe("lock:order:123:release");
    
    // 3. 알림 올 때까지 대기 (CPU 낭비 없음)
    waitForMessage();
    
    // 4. 알림 받으면 다시 획득 시도
    tryLock("order:123");
}

// 락 해제 시
unlock("order:123");
publish("lock:order:123:release", "unlocked");  // 대기자들에게 알림
```

### 5.3 클러스터에서의 분산락 동작

```
[1. 락 획득 경쟁]
앱 A, B, C: SET lock:seat:A1 NX (동시 시도)
↓
해시 슬롯 계산 → 슬롯 12345 → Node 3
↓
Node 3에만 저장 (첫 요청만 성공)
↓
앱 A: 성공 ✓
앱 B, C: 실패 ✗

[2. 대기 상태]
앱 B: SUBSCRIBE lock:seat:A1:release (Node 1에 연결)
앱 C: SUBSCRIBE lock:seat:A1:release (Node 4에 연결)

[3. 락 해제]
앱 A: DEL lock:seat:A1 (Node 3에서 삭제)
앱 A: PUBLISH lock:seat:A1:release "unlocked"
↓
Node 1 → 앱 B에게 전달 ✓
Node 4 → 앱 C에게 전달 ✓
Node 2, 5, 6 → 구독자 없음 (비효율 발생 ⚠️)

[4. 재경쟁]
앱 B, C: 다시 SET lock:seat:A1 NX
→ 앱 B 성공, 앱 C는 다시 구독
```

### 5.4 분산락에서는 왜 괜찮은가?

분산락에서도 Pub/Sub의 브로드캐스트 문제는 **여전히 존재**합니다. 하지만 실무에서 허용 가능한 이유는:

**일반 Pub/Sub (채팅, 알림):**

- 메시지 발생: 초당 수천~수만 건
- 브로드캐스트 오버헤드: **심각함** (사용 지양)

**분산락 알림 Pub/Sub:**

- 메시지 발생: 락 해제 시에만 (빈도 낮음)
- 브로드캐스트 오버헤드: **존재하지만 감당 가능** (사용 가능)

**핵심 차이: 메시지 발생 빈도**

```
실시간 좌석 현황 알림:
- 초당 1,000건 발행
- 6노드 클러스터: 6,000건 내부 전송
→ 네트워크 부하 심각 (문제!)

결제 분산락 해제 알림:
- 초당 100건 발행
- 6노드 클러스터: 600건 내부 전송
→ 네트워크 부하 감당 가능 (허용 가능)
```

**정리:** 분산락도 비효율적이지만, 빈도가 낮아서 **실무에서 견딜 만한 수준**입니다.

## 6. 그렇다면 해결 방법은

지금까지 살펴본 것처럼, Redis 클러스터에서 Pub/Sub은 심각한 비효율성을 가지고 있습니다:

**핵심 문제:**

- 모든 노드로 브로드캐스트 → 네트워크 오버헤드
- 구독자 없는 노드도 메시지 처리 → CPU/메모리 낭비
- 노드 수에 비례한 리소스 증가 → 확장성 제약

**언제 문제가 될까?**

```
트래픽이 적은 경우 (< 1,000 msg/sec):
→ 비효율은 있지만 감당 가능

트래픽이 많은 경우 (> 1,000 msg/sec):
→ 심각한 네트워크/CPU 부하
→ 비용 증가 (클라우드 환경)
→ 성능 저하 위험
```

**그렇다면 Redis는 이 문제를 어떻게 해결했을까요?**

Redis 커뮤니티와 개발팀도 이 문제를 인지하고 있었고, 여러 해결 방안을 제시했습니다:

1. **Redis 7.0의 Sharded Pub/Sub**: 채널을 해시 슬롯에 따라 분산하여 브로드캐스트 제거
2. **Redis Streams**: Pub/Sub을 대체하는 로그 기반 메시징 시스템
3. **아키텍처 분리**: Redis 클러스터와 Pub/Sub 전용 인스턴스 분리
4. **외부 메시징 시스템**: Kafka, RabbitMQ 등 전문 솔루션 활용

각 방법은 어떤 장단점이 있고, 어떤 상황에서 사용해야 할까요?

다음 글에서는 이러한 해결 방안들을 구체적으로 살펴보고, 실무에서 어떻게 선택하고 적용해야 하는지 알아보겠습니다.

---

## 핵심 요약 (TL;DR)

- **일반 데이터(SET, GET)**: 해시 슬롯에 따라 특정 노드 1개에만 저장 → 효율적 ✅
- **Pub/Sub(PUBLISH)**: 클러스터의 모든 노드로 브로드캐스트 → 비효율적 ⚠️
- **오버헤드**: 노드 수에 비례 (10개 노드 = 10배, 20개 노드 = 20배)
- **핵심 차이**: Pub/Sub은 메시지를 저장하지 않고 "통과"만 시킴
- **분산락**: Pub/Sub 사용하지만 빈도가 낮아 큰 문제 없음
- **해결책**: Sharded Pub/Sub(Redis 7.0+), Streams, 아키텍처 분리 등

## FAQ

**Q. Redis 클러스터에서 Pub/Sub을 절대 사용하면 안 되나요?**  
A. 아닙니다. 트래픽이 적다면(< 1,000 msg/sec) 충분히 사용 가능합니다. 다만 대규모 트래픽에서는 네트워크 오버헤드를 고려해야 합니다.

**Q. 분산락에서 Pub/Sub을 사용해도 괜찮은 이유는?**  
A. 락 해제 시에만 메시지를 발행하므로 빈도가 낮습니다. 일반적인 실시간 메시징에 비해 오버헤드가 훨씬 적습니다.

**Q. SET 명령어는 왜 다른 노드로 전파되지 않나요?**  
A. Redis 클러스터는 해시 슬롯 기반 샤딩을 사용합니다. 각 키는 특정 슬롯에 할당되고, 해당 슬롯을 담당하는 노드에만 저장됩니다.

**Q. 메시지 손실 없이 Pub/Sub을 사용하려면?**  
A. Pub/Sub은 구조적으로 메시지를 저장하지 않습니다. 지속성이 필요하다면 Redis Streams나 Kafka 같은 메시징 시스템을 고려하세요.

**Q. Redis 7.0의 Sharded Pub/Sub은 뭐가 다른가요?**  
A. 채널명이 해시 슬롯에 따라 특정 노드에 할당되어 브로드캐스트가 발생하지 않습니다. 다음 글에서 자세히 다룰 예정입니다.

---

## 관련 검색어

이 글과 관련된 주제들:

- Redis 클러스터 성능 최적화
- Redis Pub/Sub vs Kafka
- Redis Streams 사용법
- 분산 시스템 메시징 패턴
- Redis 해시 슬롯 동작 원리
- Redis 클러스터 아키텍처
- 실시간 메시징 시스템 구축
- Redis 7.0 Sharded Pub/Sub
- Redis 분산락 구현
- 메시지 브로커 선택 가이드

---

**다음 글 예고:**

- Redis 7.0+ Sharded Pub/Sub 완전 가이드
- Redis Streams를 활용한 메시징 패턴
- 실무 아키텍처 선택 가이드
- 트래픽별 최적 솔루션 비교

---

**참고 자료:**

- [Redis Cluster Specification](https://redis.io/docs/latest/operate/oss_and_stack/reference/cluster-spec/)
- [Redis Pub/Sub](https://redis.io/docs/latest/develop/pubsub/)
- [Scale with Redis Cluster](https://redis.io/docs/latest/operate/oss_and_stack/management/scaling/)
- [Redis Sharded Pub/Sub (Redis 7.0+)](https://redis.io/docs/latest/commands/spublish/)

---

> 💡 **이 글이 도움이 되셨나요?** 궁금한 점이나 추가로 다뤄주길 바라는 주제가 있다면 댓글로 남겨주세요!